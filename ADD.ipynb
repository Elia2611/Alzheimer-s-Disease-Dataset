{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7837da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, roc_auc_score, classification_report, make_scorer, average_precision_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ae222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carreguem el dataset i eliminem una columna no rellevant\n",
    "df = pd.read_csv(\"alzheimers_disease_data.csv\")\n",
    "alzheimer = df.drop(columns=\"DoctorInCharge\")\n",
    "alzheimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8315a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definim els models bàsics inicials\n",
    "models = {\n",
    "    \"SVM\": make_pipeline(StandardScaler(), SVC()),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Logistic Regression\": make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    \"K-Nearest Neighbors\": make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecció de les variables explicatives i la variable target\n",
    "var = [c for c in alzheimer.columns if c not in [\"Diagnosis\", \"PatientID\"]]\n",
    "\n",
    "target_variable = \"Diagnosis\"\n",
    "\n",
    "X = alzheimer[var]\n",
    "y = alzheimer[target_variable]\n",
    "\n",
    "# Divisió en entrenament / test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d41c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenament i mètrica bàsiques per a cada model\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train) \n",
    "    y_pred = model.predict(X_test) \n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1-score\": f1}\n",
    "    \n",
    "    print(f\"\\n{name} metrics:\")\n",
    "    print(f\"Accuracy:   {acc:.3f}\")\n",
    "    print(f\"Precision:  {prec:.3f}\")\n",
    "    print(f\"Recall:     {rec:.3f}\")\n",
    "    print(f\"F1-score:   {f1:.3f}\")\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b7752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validació creuada \n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "scoring = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_validate(model, X, y, cv=cv, scoring=scoring)\n",
    "    cv_results[name] = {\n",
    "        \"Accuracy\": np.mean(scores[\"test_accuracy\"]),\n",
    "        \"Precision\": np.mean(scores[\"test_precision\"]),\n",
    "        \"Recall\": np.mean(scores[\"test_recall\"]),\n",
    "        \"F1-score\": np.mean(scores[\"test_f1\"])\n",
    "    }\n",
    "\n",
    "for model_name, metrics in cv_results.items():\n",
    "    print(f\"\\n{model_name} Cross-Validation Metrics:\")\n",
    "    print(f\"Accuracy:  {metrics['Accuracy']:.3f}\")\n",
    "    print(f\"Precision: {metrics['Precision']:.3f}\")\n",
    "    print(f\"Recall:    {metrics['Recall']:.3f}\")\n",
    "    print(f\"F1-score:  {metrics['F1-score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe74f315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estabilitat del F1.score amb múltiples particions\n",
    "testsize = 0.2\n",
    "n_proves = 30\n",
    "f1_r=np.zeros((n_proves,len(models)))\n",
    "model_names = list(models.keys())\n",
    "\n",
    "for i in range(n_proves):\n",
    "    for j, name in enumerate(model_names):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testsize)\n",
    "\n",
    "        model = models[name]\n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        y_pred=model.predict(X_test)\n",
    "\n",
    "        f1_r[i][j] = f1_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,7.5))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.boxplot(f1_r)\n",
    "for i in range(len(models)):\n",
    "    xderiv = (i+1)*np.ones(f1_r[:,i].shape)+(np.random.rand(n_proves,)-0.5)*0.1\n",
    "    plt.plot(xderiv,f1_r[:,i],'ro',alpha=0.3)\n",
    "\n",
    "\n",
    "plt.xticks(range(1, len(model_names) + 1), model_names)\n",
    "plt.ylabel('F1_score', fontsize=16 )\n",
    "plt.title('F1_score (sense balancejar les dades)', fontsize=18)\n",
    "\n",
    "\n",
    "mean_f1 = f1_r.mean(axis=0)\n",
    "std_f1 = f1_r.std(axis=0)\n",
    "\n",
    "for name, mean, std in zip(model_names, mean_f1, std_f1):\n",
    "    print(f\"{name:<20} F1: {mean:.3f} ± {std:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd5e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definició de models per a cerca d'hiperparàmetres\n",
    "models = {\n",
    "    \"Logistic Regression\": Pipeline([\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression())\n",
    "    ]),\n",
    "    \n",
    "    \"KNN\": Pipeline([\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"clf\", KNeighborsClassifier())\n",
    "    ]),\n",
    "    \n",
    "    \"SVM\": Pipeline([\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"clf\", SVC())\n",
    "    ]),\n",
    "    \n",
    "    \"Random Forest\": Pipeline([\n",
    "        (\"clf\", RandomForestClassifier())\n",
    "    ]),\n",
    "    \n",
    "    \"GradientBoosting\": Pipeline([\n",
    "        (\"clf\", GradientBoostingClassifier())\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278822ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionaris d'hiperparèmetres per GridSearch / RandomSearch\n",
    "param_grids = {\n",
    "   \"Logistic Regression\": {\n",
    "        \"clf__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        \"clf__solver\": [\"lbfgs\", \"liblinear\"],\n",
    "        \"clf__max_iter\": [300, 500, 1000]\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"clf__n_neighbors\": [1, 3, 5, 7, 9, 15, 25],\n",
    "        \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "        \"clf__p\": [1, 2]   # Manhattan / Euclidean\n",
    "    },\n",
    "\n",
    "    \"SVM\": {\n",
    "        \"clf__C\": [0.01, 0.1, 1, 10, 100],\n",
    "        \"clf__kernel\": [\"linear\", \"rbf\"],\n",
    "        \"clf__gamma\": [\"scale\", \"auto\"]   # només s’usa amb RBF\n",
    "    },\n",
    "\n",
    "    \"Random Forest\": {\n",
    "        \"clf__n_estimators\": [100, 200, 400, 700],\n",
    "        \"clf__max_depth\": [None, 5, 10, 20],\n",
    "        \"clf__min_samples_split\": [2, 5, 10],\n",
    "        \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "        \"clf__max_features\": [\"sqrt\", \"log2\"]\n",
    "    },\n",
    "    \n",
    "    \"GradientBoosting\": {\n",
    "        \"clf__n_estimators\": [100, 300, 500],\n",
    "        \"clf__learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"clf__max_depth\": [2, 3, 5],\n",
    "        \"clf__subsample\": [0.6, 0.8, 1.0]\n",
    "    }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6253b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scorer basat en F1\n",
    "f1_scorer = make_scorer(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28410430",
   "metadata": {},
   "source": [
    "**Grid Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8abdaf1",
   "metadata": {},
   "source": [
    "Fem una primera cerca d’hiperparàmetres amb GridSearchCV. Aquesta funció prova totes les combinacions d’hiperparàmetres establertes al diccionari param\\_grid i, per tant, és un procés molt lent. Concretament, tarda aproximadament 62 minuts a executar-se. Per aquest motiu, hem deixat el codi comentat i hem inclòs en una taula els resultats d’una execució anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64bb295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "best_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    grid = GridSearchCV(model, param_grids[name], cv=5, scoring='f1')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    start = time.time()\n",
    "    grid.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    \n",
    "    best_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Best Params\": grid.best_params_,\n",
    "        \"Train F1\": f1_score(y_train, grid.predict(X_train)),\n",
    "        \"Test F1\": f1_score(y_test, grid.predict(X_test)),\n",
    "        \"Time (s)\": round(end - start, 2)\n",
    "    })\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea096b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(\"Resultats del Grid Search: \\n\")\n",
    "results_df = pd.DataFrame(best_results)\n",
    "print(results_df.sort_values(by=\"Test F1\", ascending=False).to_string(index=False))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bfd3d4",
   "metadata": {},
   "source": [
    "**Randomized Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff67825",
   "metadata": {},
   "source": [
    "Fem una segona cerca d’hiperparàmetres amb RandomizedSearchCV. Aquesta funció, a diferència de GridSearchCV, no prova totes les combinacions d’hiperparàmetres, sinó que selecciona combinacions aleatòries. Aquesta tècnica és més ràpida que l’anterior, però continua tenint un temps d’execució força elevat. Per aquest motiu, també hem deixat el codi comentat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"best_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    grid = RandomizedSearchCV(model, param_grids[name], cv=5, scoring='f1')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    start = time.time()\n",
    "    grid.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    \n",
    "    best_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Best Params\": grid.best_params_,\n",
    "        \"Train F1\": f1_score(y_train, grid.predict(X_train)),\n",
    "        \"Test F1\": f1_score(y_test, grid.predict(X_test)),\n",
    "        \"Time (s)\": round(end - start, 2)\n",
    "    })\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eff38c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"print(\"Resultats del Grid Search: \\n\")\n",
    "results_df = pd.DataFrame(best_results)\n",
    "print(results_df.sort_values(by=\"Test F1\", ascending=False).to_string(index=False))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df09d632",
   "metadata": {},
   "source": [
    "Resultat de la cerca d'hiperparametres amb Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695f1573",
   "metadata": {},
   "source": [
    "| Model               | Best Params                                                                                                                                    | Train F1  | Test F1  | Time (s) |\n",
    "|---------------------|------------------------------------------------------------------------------------------------------------------------------------------------|----------|----------|----------|\n",
    "| GradientBoosting    | {'clf_learning_rate': 0.01, 'clfmax_depth': 2, 'clfn_estimators': 500, 'clf_subsample': 1.0}                                             | 0.940883 | 0.914676 | 1223.48  |\n",
    "| Random Forest       | {'clf_max_depth': 20, 'clfmax_features': 'log2', 'clfmin_samples_leaf': 1, 'clfmin_samples_split': 2, 'clf_n_estimators': 400}        | 1.000000 | 0.884211 | 2463.35  |\n",
    "| SVM                 | {'clf_C': 10, 'clfgamma': 'scale', 'clf_kernel': 'linear'}                                                                                 | 0.788660 | 0.726027 | 72.01    |\n",
    "| Logistic Regression | {'clf_C': 0.001, 'clfmax_iter': 300, 'clf_solver': 'liblinear'}                                                                            | 0.778063 | 0.710345 | 1.84     |\n",
    "| KNN                 | {'clf_n_neighbors': 9, 'clfp': 1, 'clf_weights': 'uniform'}                                                                                | 0.716763 | 0.516393 | 1.87     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abfed16",
   "metadata": {},
   "source": [
    "Una primera selecció d'hiperparametres (RandomizedSearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd12b2",
   "metadata": {},
   "source": [
    "| Model               | Best Params                                                                                                                                                               | Train F1  | Test F1  | Time (s) |\n",
    "|---------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|----------|----------|\n",
    "| GradientBoosting    | {'clf__subsample': 0.8, 'clf__n_estimators': 100, 'clf__max_depth': 5, 'clf__learning_rate': 0.05}                                                                        | 0.975894 | 0.905405 | 174.62   |\n",
    "| Random Forest       | {'clf__n_estimators': 400, 'clf__min_samples_split': 5, 'clf__min_samples_leaf': 1, 'clf__max_features': 'log2', 'clf__max_depth': 20}                                   | 0.994200 | 0.891986 | 87.46    |\n",
    "| SVM                 | {'clf__kernel': 'linear', 'clf__gamma': 'auto', 'clf__C': 100}                                                                                                            | 0.789338 | 0.726027 | 69.68    |\n",
    "| Logistic Regression | {'clf__solver': 'liblinear', 'clf__max_iter': 500, 'clf__C': 0.001}                                                                                                       | 0.778063 | 0.710345 | 0.52     |\n",
    "| KNN                 | {'clf__weights': 'uniform', 'clf__p': 1, 'clf__n_neighbors': 9}                                                                                                           | 0.716763 | 0.516393 | 0.79     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e9bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models entrenats amb hiperparàmetres òptims trobats\n",
    "models = {\n",
    "    \"Logistic Regression\": make_pipeline(StandardScaler(), \n",
    "                                    LogisticRegression(solver='liblinear', max_iter=300, C=0.001)),\n",
    "    \"KNN\": make_pipeline(StandardScaler(), \n",
    "                    KNeighborsClassifier(weights='uniform', p=1, n_neighbors=9)),\n",
    "    \"SVM\": make_pipeline(StandardScaler(),\n",
    "                    SVC(probability=True, kernel='linear', gamma='scale', C=10)),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=400, min_samples_split=2, min_samples_leaf=1, max_features='log2',max_depth=20),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(subsample=0.6, n_estimators=500, max_depth=2, learning_rate=0.01)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36cadbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation amb els models optimitzats\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "scoring = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_validate(model, X, y, cv=cv, scoring=scoring)\n",
    "    cv_results[name] = {\n",
    "        \"Accuracy\": np.mean(scores[\"test_accuracy\"]),\n",
    "        \"Precision\": np.mean(scores[\"test_precision\"]),\n",
    "        \"Recall\": np.mean(scores[\"test_recall\"]),\n",
    "        \"F1-score\": np.mean(scores[\"test_f1\"])\n",
    "    }\n",
    "\n",
    "for model_name, metrics in cv_results.items():\n",
    "    print(f\"\\n{model_name} Cross-Validation Metrics:\")\n",
    "    print(f\"Accuracy:  {metrics['Accuracy']:.3f}\")\n",
    "    print(f\"Precision: {metrics['Precision']:.3f}\")\n",
    "    print(f\"Recall:    {metrics['Recall']:.3f}\")\n",
    "    print(f\"F1-score:  {metrics['F1-score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c212e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estabilitat del F1-score després de l'optimització\n",
    "testsize = 0.2\n",
    "n_proves = 30\n",
    "f1_r=np.zeros((n_proves,len(models)))\n",
    "model_names = list(models.keys())\n",
    "\n",
    "for i in range(n_proves):\n",
    "    for j, name in enumerate(model_names):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testsize)\n",
    "\n",
    "        model = models[name]\n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        y_pred=model.predict(X_test)\n",
    "\n",
    "        f1_r[i][j] = f1_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,7.5))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.boxplot(f1_r)\n",
    "for i in range(len(models)):\n",
    "    xderiv = (i+1)*np.ones(f1_r[:,i].shape)+(np.random.rand(n_proves,)-0.5)*0.1\n",
    "    plt.plot(xderiv,f1_r[:,i],'ro',alpha=0.3)\n",
    "\n",
    "\n",
    "plt.xticks(range(1, len(model_names) + 1), model_names)\n",
    "plt.ylabel('F1_score', fontsize=16  )\n",
    "plt.title('F1_score (sense balancejar les dades)',fontsize=18)\n",
    "\n",
    "\n",
    "mean_f1 = f1_r.mean(axis=0)\n",
    "std_f1 = f1_r.std(axis=0)\n",
    "\n",
    "for name, mean, std in zip(model_names, mean_f1, std_f1):\n",
    "    print(f\"{name:<20} F1: {mean:.3f} ± {std:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d9725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corbes ROC\n",
    "plt.figure()\n",
    "lw = 2\n",
    "\n",
    "print(f\"ROC Curve (AUC-ROC):\")\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "    auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "    fpr, tpr, ths = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={auc_roc:.3f})\", lw=lw)\n",
    "    print(f\"\\t{name}: {auc_roc:.4f}\")\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "plt.figure()\n",
    "lw = 2\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "    auc_pr = average_precision_score(y_test, y_pred_proba)\n",
    "    prec, recall, ths = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "    plt.plot(recall, prec, label=f\"{name} (AUC={auc_pr:.3f})\", lw=lw)\n",
    "\n",
    "\n",
    "prop = np.mean(y_test)\n",
    "plt.plot([0, 1], [prop, prop], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision Recall Curve (AUC-PR: {auc_pr:.4f})\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620fbe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importància de les variables segons GradientBoosting i RandomForest\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    subsample=1.0,\n",
    "    n_estimators=100,\n",
    "    max_depth=2,\n",
    "    learning_rate=0.05\n",
    ")\n",
    "\n",
    "gb_model.fit(X, y)\n",
    "\n",
    "importances = gb_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Importància de les variables Gradient Bossting:\")\n",
    "for idx in indices:\n",
    "    print(f\"{feature_names[idx]:<25} {importances[idx]:.4f}\")\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    max_depth=None\n",
    ")\n",
    "\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"\\nImportància de les variables Random Forest\")\n",
    "for idx in indices:\n",
    "    print(f'{feature_names[idx]:<25} {importances[idx]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t2 (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
